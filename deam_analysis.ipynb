{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "lLKYJzFwswV8",
      "metadata": {
        "id": "lLKYJzFwswV8"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "00265c9d",
      "metadata": {
        "id": "00265c9d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "import librosa\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.stats import pearsonr\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6410a9a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6410a9a1",
        "outputId": "10373898-444d-47f0-ad14-35d171f47898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1986607 samples at 44100 Hz\n"
          ]
        }
      ],
      "source": [
        "# performing a check\n",
        "id = 35\n",
        "path = f'/DEAM_Dataset/DEAM_audio/MEMD_audio/{id}.mp3'\n",
        "y, sr = librosa.load(path, sr=44100)\n",
        "print(f\"{len(y)} samples at {sr} Hz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b2255be9",
      "metadata": {
        "id": "b2255be9"
      },
      "outputs": [],
      "source": [
        "def compute_beats(y, sr, hop_length):\n",
        "\n",
        "    tempo, beat_frames = librosa.beat.beat_track(\n",
        "        y=y,\n",
        "        sr=sr,\n",
        "        hop_length=hop_length,\n",
        "        units=\"frames\"\n",
        "    )\n",
        "\n",
        "    beat_times_sec = librosa.frames_to_time(\n",
        "        beat_frames,\n",
        "        sr=sr,\n",
        "        hop_length=hop_length\n",
        "    )\n",
        "\n",
        "    return beat_times_sec, beat_frames, tempo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b24eaaaa",
      "metadata": {
        "id": "b24eaaaa"
      },
      "outputs": [],
      "source": [
        "def aggregate_feature(mfcc, cens, beat_frames, beat_times):\n",
        "\n",
        "    X = []\n",
        "    durations = []\n",
        "\n",
        "    # beat_durations = np.diff(beat_times)\n",
        "    # beat_durations = np.clip(beat_durations, 1e-2, None)\n",
        "\n",
        "    # tempo_inst = 60.0 / beat_durations\n",
        "    # tempo_features = np.column_stack([\n",
        "    #         tempo_inst,\n",
        "    #         np.log(tempo_inst),\n",
        "    #         np.diff(np.concatenate([[tempo_inst[0]], tempo_inst]))\n",
        "    #     ])\n",
        "\n",
        "    for t in range(len(beat_frames) - 1):\n",
        "        start = beat_frames[t]\n",
        "        end   = beat_frames[t + 1]\n",
        "\n",
        "        if end > start:\n",
        "            mfcc_t = mfcc[start:end].mean(axis=0)\n",
        "            cens_t = cens[start:end].mean(axis=0)\n",
        "        else:\n",
        "            mfcc_t = mfcc[start]\n",
        "            cens_t = cens[start:end]\n",
        "\n",
        "        X.append(np.concatenate([mfcc_t, cens_t]))\n",
        "        durations.append(beat_times[t + 1] - beat_times[t])\n",
        "\n",
        "\n",
        "\n",
        "    X = np.vstack(X)\n",
        "    return X, np.array(durations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8406bd5f",
      "metadata": {
        "id": "8406bd5f"
      },
      "outputs": [],
      "source": [
        "def parse_annotation_times(df):\n",
        "\n",
        "    time_cols = [c for c in df.columns if c.startswith(\"sample_\")]\n",
        "\n",
        "    times_sec = np.array([int(c.replace(\"sample_\", \"\").replace(\"ms\", \"\")) / 1000.0\n",
        "                          for c in time_cols])\n",
        "\n",
        "    return time_cols, times_sec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c5e07b74",
      "metadata": {
        "id": "c5e07b74"
      },
      "outputs": [],
      "source": [
        "def aggregate_annotations_to_beats(\n",
        "        times_sec,\n",
        "        values,\n",
        "        beat_times_sec\n",
        "):\n",
        "\n",
        "    y_beats = []\n",
        "\n",
        "    for t in range(len(beat_times_sec) - 1):\n",
        "        start = beat_times_sec[t]\n",
        "        end   = beat_times_sec[t + 1]\n",
        "\n",
        "        mask = (times_sec >= start) & (times_sec < end)\n",
        "\n",
        "        if np.any(mask):\n",
        "            y_beats.append(values[mask].mean())\n",
        "        else:\n",
        "            # fallback: nearest neighbor\n",
        "            idx = np.argmin(np.abs(times_sec - start))\n",
        "            y_beats.append(values[idx])\n",
        "    return np.array(y_beats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b9d3ed5d",
      "metadata": {
        "id": "b9d3ed5d"
      },
      "outputs": [],
      "source": [
        "def get_song_ids(filepath):\n",
        "\n",
        "    ids = []\n",
        "    for root, dirs, files in os.walk(filepath):\n",
        "        for file in files:\n",
        "            if \".mp3\" in file:\n",
        "                ids.append(file[:file.index(\".\")])\n",
        "\n",
        "    return ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e06a37c5",
      "metadata": {
        "id": "e06a37c5"
      },
      "outputs": [],
      "source": [
        "class DEAMDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            audio_dir,\n",
        "            arousal_labels,\n",
        "            valence_labels,\n",
        "            song_ids,\n",
        "            hop_length=512,\n",
        "            n_mfcc=20\n",
        "    ):\n",
        "\n",
        "        self.audio_dir = audio_dir\n",
        "        self.arousal_labels = pd.read_csv(arousal_labels)\n",
        "        self.valence_labels = pd.read_csv(valence_labels)\n",
        "        self.hop_length = hop_length\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.song_ids = song_ids\n",
        "        self.cache = {}\n",
        "        self.time_cols, self.time_sec = parse_annotation_times(self.valence_labels)\n",
        "\n",
        "        self.valence_labels[\"song_id\"] = self.valence_labels[\"song_id\"].astype(str).str.strip()\n",
        "        self.arousal_labels[\"song_id\"] = self.arousal_labels[\"song_id\"].astype(str).str.strip()\n",
        "\n",
        "        self.valence_dict = self.valence_labels.set_index(\"song_id\")\n",
        "        self.arousal_dict = self.arousal_labels.set_index(\"song_id\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.song_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        song_id = self.song_ids[idx]\n",
        "\n",
        "        if song_id in self.cache:\n",
        "          return self.cache[song_id]\n",
        "\n",
        "        path = os.path.join(self.audio_dir, f\"{song_id}.mp3\")\n",
        "\n",
        "        # y, sr = librosa.load(path, sr=44100)\n",
        "\n",
        "        # beat_times_sec, beat_frames, tempo = compute_beats(y, sr, self.hop_length)\n",
        "\n",
        "        y, sr = librosa.load(path, sr=22050, mono=True)\n",
        "\n",
        "        tempo, beat_frames = librosa.beat.beat_track(\n",
        "          y=y, sr=sr, hop_length=self.hop_length, units=\"frames\"\n",
        "        )\n",
        "\n",
        "        beat_times_sec = librosa.frames_to_time(beat_frames, sr=sr, hop_length=self.hop_length)\n",
        "\n",
        "        if beat_times_sec is None or len(beat_times_sec) < 2:\n",
        "          return None\n",
        "\n",
        "        mfcc = librosa.feature.mfcc(\n",
        "            y=y,\n",
        "            sr=sr,\n",
        "            n_mfcc=self.n_mfcc,\n",
        "            hop_length=self.hop_length\n",
        "        ).T\n",
        "\n",
        "        # y_harmonic, _ = librosa.effects.hpss(y)\n",
        "\n",
        "        # hpcp = librosa.feature.chroma_cqt(\n",
        "        #     y=y_harmonic,\n",
        "        #     sr=sr,\n",
        "        #     bins_per_octave=36,\n",
        "        #     n_chroma=12\n",
        "        # ).T\n",
        "\n",
        "        cens = librosa.feature.chroma_cens(\n",
        "            y=y,\n",
        "            sr=sr,\n",
        "            hop_length=self.hop_length\n",
        "        ).T\n",
        "\n",
        "        X, durations = aggregate_feature(mfcc, cens, beat_frames, beat_times_sec)\n",
        "\n",
        "        valence = self.valence_dict.loc[str(song_id), self.time_cols].values\n",
        "        arousal = self.arousal_dict.loc[str(song_id), self.time_cols].values\n",
        "\n",
        "        val_beats = aggregate_annotations_to_beats(self.time_sec, valence, beat_times_sec)\n",
        "        aro_beats = aggregate_annotations_to_beats(self.time_sec, arousal, beat_times_sec)\n",
        "\n",
        "        X = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "        y = np.column_stack([val_beats, aro_beats])\n",
        "\n",
        "        valid = ~np.isnan(y).any(axis=1)\n",
        "\n",
        "        X = X[valid]\n",
        "        y = y[valid]\n",
        "        durations = durations[valid]\n",
        "\n",
        "        y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "        durations = torch.tensor(durations, dtype=torch.float32)\n",
        "\n",
        "        if X.shape[0] == 0 or y.shape[0] == 0:\n",
        "          return None\n",
        "\n",
        "        if not torch.isfinite(X).all():\n",
        "          return None\n",
        "        if not torch.isfinite(y).all():\n",
        "          return None\n",
        "\n",
        "        sample = {\n",
        "            \"X\": X,\n",
        "            \"y\": y,\n",
        "            \"durations\": durations\n",
        "        }\n",
        "\n",
        "        self.cache[song_id] = sample\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b119080",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b119080",
        "outputId": "30bcb7bf-4755-452b-8e12-e3f23d33de9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precomputing features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1802/1802 [33:10<00:00,  1.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ids = get_song_ids(\"/DEAM_Dataset/DEAM_audio/MEMD_audio\")\n",
        "\n",
        "dataset = DEAMDataset(\"/DEAM_Dataset/DEAM_audio/MEMD_audio\",\n",
        "                           \"/DEAM_Dataset/DEAM_Annotations/annotations/annotations averaged per song/dynamic (per second annotations)/arousal.csv\",\n",
        "                           \"/DEAM_Dataset/DEAM_Annotations/annotations/annotations averaged per song/dynamic (per second annotations)/valence.csv\",\n",
        "                           ids)\n",
        "\n",
        "print(\"Precomputing features...\")\n",
        "for i in tqdm(range(len(dataset))):\n",
        "    _ = dataset[i]\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "136a8a7b",
      "metadata": {
        "id": "136a8a7b"
      },
      "outputs": [],
      "source": [
        "class BeatRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "\n",
        "        super().__init__()\n",
        "        self.next = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.next(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "776a71ab",
      "metadata": {
        "id": "776a71ab"
      },
      "outputs": [],
      "source": [
        "def flatten_collate(batch):\n",
        "\n",
        "    batch = [b for b in batch if b is not None]\n",
        "\n",
        "    if len(batch) == 0:\n",
        "        return None, None\n",
        "\n",
        "    X_list = [b[\"X\"] for b in batch]\n",
        "    y_list = [b[\"y\"] for b in batch]\n",
        "\n",
        "    X = torch.cat(X_list, dim=0)\n",
        "    y = torch.cat(y_list, dim=0)\n",
        "\n",
        "    return X, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e179c61f",
      "metadata": {
        "id": "e179c61f"
      },
      "outputs": [],
      "source": [
        "num_songs = len(dataset)\n",
        "indices = np.arange(num_songs)\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42175ed",
      "metadata": {
        "id": "d42175ed"
      },
      "outputs": [],
      "source": [
        "def compute_normalization(loader):\n",
        "    X_all = []\n",
        "\n",
        "    for X, _ in loader:\n",
        "        if X is None:\n",
        "            continue\n",
        "        X_all.append(X)\n",
        "\n",
        "    if len(X_all) == 0:\n",
        "        raise RuntimeError(\"No valid samples found\")\n",
        "\n",
        "    X_all = torch.cat(X_all, dim=0)\n",
        "    mean = X_all.mean(dim=0)\n",
        "    std = X_all.std(dim=0) + 1e-6\n",
        "    return mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a8dc4911",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8dc4911",
        "outputId": "3defa827-3682-4777-84bd-43b88899f868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Fold 1 of 10 =====\n",
            "Epoch 01 | Train MSE: 0.0518 | Val MSE: 0.0454\n",
            "Epoch 02 | Train MSE: 0.0458 | Val MSE: 0.0423\n",
            "Epoch 03 | Train MSE: 0.0439 | Val MSE: 0.0395\n",
            "Epoch 04 | Train MSE: 0.0437 | Val MSE: 0.0405\n",
            "Epoch 05 | Train MSE: 0.0428 | Val MSE: 0.0409\n",
            "Epoch 06 | Train MSE: 0.0418 | Val MSE: 0.0414\n",
            "Epoch 07 | Train MSE: 0.0413 | Val MSE: 0.0405\n",
            "Epoch 08 | Train MSE: 0.0407 | Val MSE: 0.0410\n",
            "Epoch 09 | Train MSE: 0.0405 | Val MSE: 0.0398\n",
            "Epoch 10 | Train MSE: 0.0403 | Val MSE: 0.0406\n",
            "Epoch 11 | Train MSE: 0.0395 | Val MSE: 0.0413\n",
            "Epoch 12 | Train MSE: 0.0394 | Val MSE: 0.0404\n",
            "Epoch 13 | Train MSE: 0.0392 | Val MSE: 0.0415\n",
            "Epoch 14 | Train MSE: 0.0391 | Val MSE: 0.0417\n",
            "Epoch 15 | Train MSE: 0.0382 | Val MSE: 0.0409\n",
            "Epoch 16 | Train MSE: 0.0382 | Val MSE: 0.0413\n",
            "Epoch 17 | Train MSE: 0.0378 | Val MSE: 0.0404\n",
            "Epoch 18 | Train MSE: 0.0373 | Val MSE: 0.0407\n",
            "Epoch 19 | Train MSE: 0.0373 | Val MSE: 0.0395\n",
            "Epoch 20 | Train MSE: 0.0369 | Val MSE: 0.0422\n",
            "Epoch 21 | Train MSE: 0.0368 | Val MSE: 0.0397\n",
            "Epoch 22 | Train MSE: 0.0367 | Val MSE: 0.0408\n",
            "Epoch 23 | Train MSE: 0.0364 | Val MSE: 0.0399\n",
            "Epoch 24 | Train MSE: 0.0362 | Val MSE: 0.0400\n",
            "Epoch 25 | Train MSE: 0.0358 | Val MSE: 0.0432\n",
            "Epoch 26 | Train MSE: 0.0358 | Val MSE: 0.0401\n",
            "Epoch 27 | Train MSE: 0.0357 | Val MSE: 0.0392\n",
            "Epoch 28 | Train MSE: 0.0354 | Val MSE: 0.0398\n",
            "Epoch 29 | Train MSE: 0.0353 | Val MSE: 0.0396\n",
            "Epoch 30 | Train MSE: 0.0355 | Val MSE: 0.0409\n",
            "===== Fold 2 of 10 =====\n",
            "Epoch 01 | Train MSE: 0.0515 | Val MSE: 0.0506\n",
            "Epoch 02 | Train MSE: 0.0438 | Val MSE: 0.0502\n",
            "Epoch 03 | Train MSE: 0.0434 | Val MSE: 0.0496\n",
            "Epoch 04 | Train MSE: 0.0424 | Val MSE: 0.0487\n",
            "Epoch 05 | Train MSE: 0.0408 | Val MSE: 0.0510\n",
            "Epoch 06 | Train MSE: 0.0412 | Val MSE: 0.0483\n",
            "Epoch 07 | Train MSE: 0.0408 | Val MSE: 0.0485\n",
            "Epoch 08 | Train MSE: 0.0403 | Val MSE: 0.0487\n",
            "Epoch 09 | Train MSE: 0.0394 | Val MSE: 0.0474\n",
            "Epoch 10 | Train MSE: 0.0395 | Val MSE: 0.0480\n",
            "Epoch 11 | Train MSE: 0.0393 | Val MSE: 0.0491\n",
            "Epoch 12 | Train MSE: 0.0385 | Val MSE: 0.0481\n",
            "Epoch 13 | Train MSE: 0.0383 | Val MSE: 0.0472\n",
            "Epoch 14 | Train MSE: 0.0379 | Val MSE: 0.0501\n",
            "Epoch 15 | Train MSE: 0.0377 | Val MSE: 0.0482\n",
            "Epoch 16 | Train MSE: 0.0377 | Val MSE: 0.0494\n",
            "Epoch 17 | Train MSE: 0.0366 | Val MSE: 0.0481\n",
            "Epoch 18 | Train MSE: 0.0365 | Val MSE: 0.0495\n",
            "Epoch 19 | Train MSE: 0.0364 | Val MSE: 0.0470\n",
            "Epoch 20 | Train MSE: 0.0358 | Val MSE: 0.0480\n",
            "Epoch 21 | Train MSE: 0.0361 | Val MSE: 0.0483\n",
            "Epoch 22 | Train MSE: 0.0356 | Val MSE: 0.0483\n",
            "Epoch 23 | Train MSE: 0.0358 | Val MSE: 0.0488\n",
            "Epoch 24 | Train MSE: 0.0355 | Val MSE: 0.0473\n",
            "Epoch 25 | Train MSE: 0.0349 | Val MSE: 0.0473\n",
            "Epoch 26 | Train MSE: 0.0350 | Val MSE: 0.0475\n",
            "Epoch 27 | Train MSE: 0.0344 | Val MSE: 0.0476\n",
            "Epoch 28 | Train MSE: 0.0345 | Val MSE: 0.0478\n",
            "Epoch 29 | Train MSE: 0.0344 | Val MSE: 0.0478\n",
            "Epoch 30 | Train MSE: 0.0343 | Val MSE: 0.0489\n",
            "===== Fold 3 of 10 =====\n",
            "Epoch 01 | Train MSE: 0.0511 | Val MSE: 0.0431\n",
            "Epoch 02 | Train MSE: 0.0451 | Val MSE: 0.0410\n",
            "Epoch 03 | Train MSE: 0.0442 | Val MSE: 0.0410\n",
            "Epoch 04 | Train MSE: 0.0427 | Val MSE: 0.0410\n",
            "Epoch 05 | Train MSE: 0.0426 | Val MSE: 0.0401\n",
            "Epoch 06 | Train MSE: 0.0415 | Val MSE: 0.0397\n",
            "Epoch 07 | Train MSE: 0.0409 | Val MSE: 0.0400\n",
            "Epoch 08 | Train MSE: 0.0408 | Val MSE: 0.0399\n",
            "Epoch 09 | Train MSE: 0.0400 | Val MSE: 0.0420\n",
            "Epoch 10 | Train MSE: 0.0391 | Val MSE: 0.0405\n",
            "Epoch 11 | Train MSE: 0.0386 | Val MSE: 0.0410\n",
            "Epoch 12 | Train MSE: 0.0385 | Val MSE: 0.0395\n",
            "Epoch 13 | Train MSE: 0.0383 | Val MSE: 0.0405\n",
            "Epoch 14 | Train MSE: 0.0380 | Val MSE: 0.0399\n",
            "Epoch 15 | Train MSE: 0.0381 | Val MSE: 0.0409\n",
            "Epoch 16 | Train MSE: 0.0375 | Val MSE: 0.0402\n",
            "Epoch 17 | Train MSE: 0.0375 | Val MSE: 0.0401\n",
            "Epoch 18 | Train MSE: 0.0369 | Val MSE: 0.0403\n",
            "Epoch 19 | Train MSE: 0.0362 | Val MSE: 0.0402\n",
            "Epoch 20 | Train MSE: 0.0369 | Val MSE: 0.0402\n",
            "Epoch 21 | Train MSE: 0.0361 | Val MSE: 0.0406\n",
            "Epoch 22 | Train MSE: 0.0356 | Val MSE: 0.0408\n",
            "Epoch 23 | Train MSE: 0.0358 | Val MSE: 0.0429\n",
            "Epoch 24 | Train MSE: 0.0355 | Val MSE: 0.0406\n",
            "Epoch 25 | Train MSE: 0.0356 | Val MSE: 0.0395\n",
            "Epoch 26 | Train MSE: 0.0352 | Val MSE: 0.0413\n",
            "Epoch 27 | Train MSE: 0.0347 | Val MSE: 0.0393\n",
            "Epoch 28 | Train MSE: 0.0346 | Val MSE: 0.0400\n",
            "Epoch 29 | Train MSE: 0.0344 | Val MSE: 0.0406\n",
            "Epoch 30 | Train MSE: 0.0344 | Val MSE: 0.0408\n",
            "===== Fold 4 of 10 =====\n",
            "Epoch 01 | Train MSE: 0.0510 | Val MSE: 0.0419\n",
            "Epoch 02 | Train MSE: 0.0453 | Val MSE: 0.0402\n",
            "Epoch 03 | Train MSE: 0.0443 | Val MSE: 0.0410\n",
            "Epoch 04 | Train MSE: 0.0430 | Val MSE: 0.0402\n",
            "Epoch 05 | Train MSE: 0.0424 | Val MSE: 0.0411\n",
            "Epoch 06 | Train MSE: 0.0417 | Val MSE: 0.0416\n",
            "Epoch 07 | Train MSE: 0.0417 | Val MSE: 0.0400\n",
            "Epoch 08 | Train MSE: 0.0406 | Val MSE: 0.0398\n",
            "Epoch 09 | Train MSE: 0.0406 | Val MSE: 0.0407\n",
            "Epoch 10 | Train MSE: 0.0405 | Val MSE: 0.0403\n",
            "Epoch 11 | Train MSE: 0.0402 | Val MSE: 0.0404\n",
            "Epoch 12 | Train MSE: 0.0397 | Val MSE: 0.0405\n",
            "Epoch 13 | Train MSE: 0.0397 | Val MSE: 0.0409\n",
            "Epoch 14 | Train MSE: 0.0391 | Val MSE: 0.0396\n",
            "Epoch 15 | Train MSE: 0.0385 | Val MSE: 0.0397\n",
            "Epoch 16 | Train MSE: 0.0383 | Val MSE: 0.0399\n",
            "Epoch 17 | Train MSE: 0.0382 | Val MSE: 0.0406\n",
            "Epoch 18 | Train MSE: 0.0377 | Val MSE: 0.0401\n",
            "Epoch 19 | Train MSE: 0.0373 | Val MSE: 0.0408\n",
            "Epoch 20 | Train MSE: 0.0370 | Val MSE: 0.0404\n",
            "Epoch 21 | Train MSE: 0.0370 | Val MSE: 0.0392\n",
            "Epoch 22 | Train MSE: 0.0371 | Val MSE: 0.0396\n",
            "Epoch 23 | Train MSE: 0.0363 | Val MSE: 0.0403\n",
            "Epoch 24 | Train MSE: 0.0364 | Val MSE: 0.0398\n",
            "Epoch 25 | Train MSE: 0.0363 | Val MSE: 0.0412\n",
            "Epoch 26 | Train MSE: 0.0359 | Val MSE: 0.0393\n",
            "Epoch 27 | Train MSE: 0.0358 | Val MSE: 0.0414\n",
            "Epoch 28 | Train MSE: 0.0355 | Val MSE: 0.0408\n",
            "Epoch 29 | Train MSE: 0.0353 | Val MSE: 0.0411\n",
            "Epoch 30 | Train MSE: 0.0350 | Val MSE: 0.0401\n",
            "===== Fold 5 of 10 =====\n",
            "Epoch 01 | Train MSE: 0.0500 | Val MSE: 0.0500\n",
            "Epoch 02 | Train MSE: 0.0444 | Val MSE: 0.0491\n",
            "Epoch 03 | Train MSE: 0.0431 | Val MSE: 0.0467\n",
            "Epoch 04 | Train MSE: 0.0423 | Val MSE: 0.0468\n",
            "Epoch 05 | Train MSE: 0.0414 | Val MSE: 0.0468\n",
            "Epoch 06 | Train MSE: 0.0412 | Val MSE: 0.0468\n",
            "Epoch 07 | Train MSE: 0.0410 | Val MSE: 0.0464\n",
            "Epoch 08 | Train MSE: 0.0405 | Val MSE: 0.0468\n",
            "Epoch 09 | Train MSE: 0.0397 | Val MSE: 0.0462\n",
            "Epoch 10 | Train MSE: 0.0399 | Val MSE: 0.0457\n",
            "Epoch 11 | Train MSE: 0.0391 | Val MSE: 0.0466\n",
            "Epoch 12 | Train MSE: 0.0389 | Val MSE: 0.0473\n",
            "Epoch 13 | Train MSE: 0.0386 | Val MSE: 0.0462\n",
            "Epoch 14 | Train MSE: 0.0377 | Val MSE: 0.0460\n",
            "Epoch 15 | Train MSE: 0.0377 | Val MSE: 0.0449\n",
            "Epoch 16 | Train MSE: 0.0374 | Val MSE: 0.0466\n",
            "Epoch 17 | Train MSE: 0.0372 | Val MSE: 0.0464\n",
            "Epoch 18 | Train MSE: 0.0366 | Val MSE: 0.0459\n",
            "Epoch 19 | Train MSE: 0.0370 | Val MSE: 0.0458\n",
            "Epoch 20 | Train MSE: 0.0364 | Val MSE: 0.0463\n",
            "Epoch 21 | Train MSE: 0.0363 | Val MSE: 0.0448\n",
            "Epoch 22 | Train MSE: 0.0360 | Val MSE: 0.0461\n",
            "Epoch 23 | Train MSE: 0.0359 | Val MSE: 0.0463\n",
            "Epoch 24 | Train MSE: 0.0353 | Val MSE: 0.0468\n",
            "Epoch 25 | Train MSE: 0.0347 | Val MSE: 0.0477\n",
            "Epoch 26 | Train MSE: 0.0351 | Val MSE: 0.0459\n",
            "Epoch 27 | Train MSE: 0.0350 | Val MSE: 0.0471\n",
            "Epoch 28 | Train MSE: 0.0345 | Val MSE: 0.0459\n",
            "Epoch 29 | Train MSE: 0.0342 | Val MSE: 0.0460\n",
            "Epoch 30 | Train MSE: 0.0343 | Val MSE: 0.0467\n",
            "===== Fold 6 of 10 =====\n",
            "Epoch 01 | Train MSE: 0.0502 | Val MSE: 0.0425\n",
            "Epoch 02 | Train MSE: 0.0452 | Val MSE: 0.0406\n",
            "Epoch 03 | Train MSE: 0.0438 | Val MSE: 0.0394\n",
            "Epoch 04 | Train MSE: 0.0426 | Val MSE: 0.0404\n",
            "Epoch 05 | Train MSE: 0.0420 | Val MSE: 0.0395\n",
            "Epoch 06 | Train MSE: 0.0414 | Val MSE: 0.0403\n",
            "Epoch 07 | Train MSE: 0.0411 | Val MSE: 0.0390\n",
            "Epoch 08 | Train MSE: 0.0401 | Val MSE: 0.0396\n",
            "Epoch 09 | Train MSE: 0.0404 | Val MSE: 0.0391\n",
            "Epoch 10 | Train MSE: 0.0396 | Val MSE: 0.0410\n",
            "Epoch 11 | Train MSE: 0.0395 | Val MSE: 0.0391\n",
            "Epoch 12 | Train MSE: 0.0391 | Val MSE: 0.0403\n",
            "Epoch 13 | Train MSE: 0.0384 | Val MSE: 0.0405\n",
            "Epoch 14 | Train MSE: 0.0383 | Val MSE: 0.0439\n",
            "Epoch 15 | Train MSE: 0.0382 | Val MSE: 0.0424\n",
            "Epoch 16 | Train MSE: 0.0379 | Val MSE: 0.0399\n",
            "Epoch 17 | Train MSE: 0.0375 | Val MSE: 0.0407\n",
            "Epoch 18 | Train MSE: 0.0375 | Val MSE: 0.0403\n",
            "Epoch 19 | Train MSE: 0.0370 | Val MSE: 0.0403\n",
            "Epoch 20 | Train MSE: 0.0367 | Val MSE: 0.0409\n",
            "Epoch 21 | Train MSE: 0.0366 | Val MSE: 0.0405\n",
            "Epoch 22 | Train MSE: 0.0365 | Val MSE: 0.0398\n",
            "Epoch 23 | Train MSE: 0.0358 | Val MSE: 0.0405\n",
            "Epoch 24 | Train MSE: 0.0356 | Val MSE: 0.0414\n",
            "Epoch 25 | Train MSE: 0.0358 | Val MSE: 0.0414\n",
            "Epoch 26 | Train MSE: 0.0354 | Val MSE: 0.0428\n",
            "Epoch 27 | Train MSE: 0.0352 | Val MSE: 0.0406\n",
            "Epoch 28 | Train MSE: 0.0347 | Val MSE: 0.0405\n",
            "Epoch 29 | Train MSE: 0.0347 | Val MSE: 0.0400\n",
            "Epoch 30 | Train MSE: 0.0349 | Val MSE: 0.0395\n",
            "===== Fold 7 of 10 =====\n",
            "Epoch 01 | Train MSE: 0.0505 | Val MSE: 0.0455\n",
            "Epoch 02 | Train MSE: 0.0444 | Val MSE: 0.0461\n",
            "Epoch 03 | Train MSE: 0.0437 | Val MSE: 0.0443\n",
            "Epoch 04 | Train MSE: 0.0424 | Val MSE: 0.0453\n",
            "Epoch 05 | Train MSE: 0.0422 | Val MSE: 0.0454\n",
            "Epoch 06 | Train MSE: 0.0412 | Val MSE: 0.0457\n",
            "Epoch 07 | Train MSE: 0.0407 | Val MSE: 0.0441\n",
            "Epoch 08 | Train MSE: 0.0403 | Val MSE: 0.0455\n",
            "Epoch 09 | Train MSE: 0.0401 | Val MSE: 0.0439\n",
            "Epoch 10 | Train MSE: 0.0394 | Val MSE: 0.0446\n",
            "Epoch 11 | Train MSE: 0.0394 | Val MSE: 0.0448\n",
            "Epoch 12 | Train MSE: 0.0387 | Val MSE: 0.0442\n",
            "Epoch 13 | Train MSE: 0.0383 | Val MSE: 0.0442\n",
            "Epoch 14 | Train MSE: 0.0388 | Val MSE: 0.0467\n",
            "Epoch 15 | Train MSE: 0.0382 | Val MSE: 0.0455\n",
            "Epoch 16 | Train MSE: 0.0379 | Val MSE: 0.0448\n",
            "Epoch 17 | Train MSE: 0.0373 | Val MSE: 0.0449\n",
            "Epoch 18 | Train MSE: 0.0364 | Val MSE: 0.0445\n",
            "Epoch 19 | Train MSE: 0.0361 | Val MSE: 0.0464\n",
            "Epoch 20 | Train MSE: 0.0362 | Val MSE: 0.0470\n",
            "Epoch 21 | Train MSE: 0.0367 | Val MSE: 0.0446\n",
            "Epoch 22 | Train MSE: 0.0359 | Val MSE: 0.0460\n",
            "Epoch 23 | Train MSE: 0.0356 | Val MSE: 0.0454\n",
            "Epoch 24 | Train MSE: 0.0358 | Val MSE: 0.0450\n",
            "Epoch 25 | Train MSE: 0.0357 | Val MSE: 0.0470\n",
            "Epoch 26 | Train MSE: 0.0352 | Val MSE: 0.0464\n",
            "Epoch 27 | Train MSE: 0.0349 | Val MSE: 0.0459\n",
            "Epoch 28 | Train MSE: 0.0346 | Val MSE: 0.0452\n",
            "Epoch 29 | Train MSE: 0.0345 | Val MSE: 0.0466\n",
            "Epoch 30 | Train MSE: 0.0342 | Val MSE: 0.0447\n",
            "===== Fold 8 of 10 =====\n",
            "Epoch 01 | Train MSE: 0.0489 | Val MSE: 0.0475\n",
            "Epoch 02 | Train MSE: 0.0446 | Val MSE: 0.0454\n",
            "Epoch 03 | Train MSE: 0.0440 | Val MSE: 0.0444\n",
            "Epoch 04 | Train MSE: 0.0426 | Val MSE: 0.0438\n",
            "Epoch 05 | Train MSE: 0.0416 | Val MSE: 0.0434\n",
            "Epoch 06 | Train MSE: 0.0418 | Val MSE: 0.0451\n",
            "Epoch 07 | Train MSE: 0.0414 | Val MSE: 0.0439\n",
            "Epoch 08 | Train MSE: 0.0405 | Val MSE: 0.0430\n",
            "Epoch 09 | Train MSE: 0.0401 | Val MSE: 0.0437\n",
            "Epoch 10 | Train MSE: 0.0394 | Val MSE: 0.0433\n",
            "Epoch 11 | Train MSE: 0.0397 | Val MSE: 0.0424\n",
            "Epoch 12 | Train MSE: 0.0396 | Val MSE: 0.0427\n",
            "Epoch 13 | Train MSE: 0.0389 | Val MSE: 0.0433\n",
            "Epoch 14 | Train MSE: 0.0386 | Val MSE: 0.0426\n",
            "Epoch 15 | Train MSE: 0.0380 | Val MSE: 0.0418\n",
            "Epoch 16 | Train MSE: 0.0381 | Val MSE: 0.0422\n",
            "Epoch 17 | Train MSE: 0.0379 | Val MSE: 0.0416\n",
            "Epoch 18 | Train MSE: 0.0373 | Val MSE: 0.0416\n",
            "Epoch 19 | Train MSE: 0.0375 | Val MSE: 0.0421\n",
            "Epoch 20 | Train MSE: 0.0367 | Val MSE: 0.0423\n",
            "Epoch 21 | Train MSE: 0.0362 | Val MSE: 0.0423\n",
            "Epoch 22 | Train MSE: 0.0365 | Val MSE: 0.0422\n",
            "Epoch 23 | Train MSE: 0.0363 | Val MSE: 0.0431\n",
            "Epoch 24 | Train MSE: 0.0359 | Val MSE: 0.0429\n",
            "Epoch 25 | Train MSE: 0.0358 | Val MSE: 0.0435\n",
            "Epoch 26 | Train MSE: 0.0355 | Val MSE: 0.0424\n",
            "Epoch 27 | Train MSE: 0.0352 | Val MSE: 0.0414\n",
            "Epoch 28 | Train MSE: 0.0351 | Val MSE: 0.0417\n",
            "Epoch 29 | Train MSE: 0.0349 | Val MSE: 0.0421\n",
            "Epoch 30 | Train MSE: 0.0350 | Val MSE: 0.0419\n",
            "===== Fold 9 of 10 =====\n",
            "Epoch 01 | Train MSE: 0.0506 | Val MSE: 0.0447\n",
            "Epoch 02 | Train MSE: 0.0451 | Val MSE: 0.0432\n",
            "Epoch 03 | Train MSE: 0.0442 | Val MSE: 0.0430\n",
            "Epoch 04 | Train MSE: 0.0430 | Val MSE: 0.0422\n",
            "Epoch 05 | Train MSE: 0.0423 | Val MSE: 0.0428\n",
            "Epoch 06 | Train MSE: 0.0419 | Val MSE: 0.0434\n",
            "Epoch 07 | Train MSE: 0.0408 | Val MSE: 0.0418\n",
            "Epoch 08 | Train MSE: 0.0407 | Val MSE: 0.0420\n",
            "Epoch 09 | Train MSE: 0.0397 | Val MSE: 0.0452\n",
            "Epoch 10 | Train MSE: 0.0398 | Val MSE: 0.0430\n",
            "Epoch 11 | Train MSE: 0.0393 | Val MSE: 0.0434\n",
            "Epoch 12 | Train MSE: 0.0394 | Val MSE: 0.0441\n",
            "Epoch 13 | Train MSE: 0.0393 | Val MSE: 0.0435\n",
            "Epoch 14 | Train MSE: 0.0384 | Val MSE: 0.0445\n",
            "Epoch 15 | Train MSE: 0.0381 | Val MSE: 0.0415\n",
            "Epoch 16 | Train MSE: 0.0378 | Val MSE: 0.0439\n",
            "Epoch 17 | Train MSE: 0.0378 | Val MSE: 0.0422\n",
            "Epoch 18 | Train MSE: 0.0373 | Val MSE: 0.0426\n",
            "Epoch 19 | Train MSE: 0.0368 | Val MSE: 0.0426\n",
            "Epoch 20 | Train MSE: 0.0366 | Val MSE: 0.0426\n",
            "Epoch 21 | Train MSE: 0.0364 | Val MSE: 0.0421\n",
            "Epoch 22 | Train MSE: 0.0362 | Val MSE: 0.0426\n",
            "Epoch 23 | Train MSE: 0.0360 | Val MSE: 0.0436\n",
            "Epoch 24 | Train MSE: 0.0354 | Val MSE: 0.0438\n",
            "Epoch 25 | Train MSE: 0.0358 | Val MSE: 0.0429\n",
            "Epoch 26 | Train MSE: 0.0356 | Val MSE: 0.0440\n",
            "Epoch 27 | Train MSE: 0.0357 | Val MSE: 0.0424\n",
            "Epoch 28 | Train MSE: 0.0352 | Val MSE: 0.0428\n",
            "Epoch 29 | Train MSE: 0.0350 | Val MSE: 0.0433\n",
            "Epoch 30 | Train MSE: 0.0348 | Val MSE: 0.0431\n",
            "===== Fold 10 of 10 =====\n",
            "Epoch 01 | Train MSE: 0.0531 | Val MSE: 0.0374\n",
            "Epoch 02 | Train MSE: 0.0469 | Val MSE: 0.0367\n",
            "Epoch 03 | Train MSE: 0.0452 | Val MSE: 0.0373\n",
            "Epoch 04 | Train MSE: 0.0444 | Val MSE: 0.0362\n",
            "Epoch 05 | Train MSE: 0.0433 | Val MSE: 0.0396\n",
            "Epoch 06 | Train MSE: 0.0418 | Val MSE: 0.0369\n",
            "Epoch 07 | Train MSE: 0.0424 | Val MSE: 0.0404\n",
            "Epoch 08 | Train MSE: 0.0418 | Val MSE: 0.0369\n",
            "Epoch 09 | Train MSE: 0.0409 | Val MSE: 0.0367\n",
            "Epoch 10 | Train MSE: 0.0407 | Val MSE: 0.0378\n",
            "Epoch 11 | Train MSE: 0.0405 | Val MSE: 0.0372\n",
            "Epoch 12 | Train MSE: 0.0398 | Val MSE: 0.0370\n",
            "Epoch 13 | Train MSE: 0.0389 | Val MSE: 0.0367\n",
            "Epoch 14 | Train MSE: 0.0389 | Val MSE: 0.0375\n",
            "Epoch 15 | Train MSE: 0.0388 | Val MSE: 0.0379\n",
            "Epoch 16 | Train MSE: 0.0389 | Val MSE: 0.0372\n",
            "Epoch 17 | Train MSE: 0.0382 | Val MSE: 0.0360\n",
            "Epoch 18 | Train MSE: 0.0381 | Val MSE: 0.0376\n",
            "Epoch 19 | Train MSE: 0.0373 | Val MSE: 0.0371\n",
            "Epoch 20 | Train MSE: 0.0374 | Val MSE: 0.0360\n",
            "Epoch 21 | Train MSE: 0.0371 | Val MSE: 0.0361\n",
            "Epoch 22 | Train MSE: 0.0368 | Val MSE: 0.0388\n",
            "Epoch 23 | Train MSE: 0.0366 | Val MSE: 0.0371\n",
            "Epoch 24 | Train MSE: 0.0368 | Val MSE: 0.0362\n",
            "Epoch 25 | Train MSE: 0.0361 | Val MSE: 0.0373\n",
            "Epoch 26 | Train MSE: 0.0362 | Val MSE: 0.0388\n",
            "Epoch 27 | Train MSE: 0.0359 | Val MSE: 0.0377\n",
            "Epoch 28 | Train MSE: 0.0355 | Val MSE: 0.0383\n",
            "Epoch 29 | Train MSE: 0.0357 | Val MSE: 0.0381\n",
            "Epoch 30 | Train MSE: 0.0353 | Val MSE: 0.0373\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# valid = 0\n",
        "# invalid = 0\n",
        "\n",
        "# for i in range(len(dataset)):\n",
        "#     sample = dataset[i]\n",
        "#     if sample is None:\n",
        "#         invalid += 1\n",
        "#     else:\n",
        "#         valid += 1\n",
        "\n",
        "# print(\"Valid songs:\", valid)\n",
        "# print(\"Invalid songs:\", invalid)\n",
        "\n",
        "fold_results = []\n",
        "val_pear_results = []\n",
        "aro_pear_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n",
        "    print(f\"===== Fold {fold + 1} of 10 =====\")\n",
        "\n",
        "    train_set = Subset(dataset, train_idx)\n",
        "\n",
        "    # for i in range(len(train_set)):\n",
        "    #     try:\n",
        "    #         sample = train_set[i]\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error at train_set index {i}\")\n",
        "    #         raise\n",
        "\n",
        "\n",
        "    val_set = Subset(dataset, val_idx)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_set,\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "        collate_fn=flatten_collate\n",
        "    )\n",
        "\n",
        "    # for i, (X, y) in enumerate(train_loader):\n",
        "    #     print(i, X.shape)\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        dataset=val_set,\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        collate_fn=flatten_collate\n",
        "    )\n",
        "\n",
        "    # batch = next(iter(train_loader))\n",
        "    # print(type(batch))\n",
        "    # print(batch)\n",
        "\n",
        "    mean, std = compute_normalization(train_loader)\n",
        "\n",
        "    input_dim = train_set[0][\"X\"].shape[1]\n",
        "\n",
        "    model = BeatRegressor(input_dim).to(device)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_val_pear = float(\"inf\")\n",
        "    best_aro_pear = float(\"inf\")\n",
        "\n",
        "    for epoch in range(30):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        train_corr_valence = 0.0\n",
        "        train_corr_arousal = 0.0\n",
        "\n",
        "        n_batches = 0\n",
        "\n",
        "        for X, y in train_loader:\n",
        "\n",
        "            X = ((X - mean) / std).to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(X)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            corr_valence, _ = pearsonr(y[:, 0].cpu().detach().numpy(), y_hat[:, 0].cpu().detach().numpy())\n",
        "            corr_arousal, _ = pearsonr(y[:, 1].cpu().detach().numpy(), y_hat[:, 1].cpu().detach().numpy())\n",
        "            n_batches += 1\n",
        "\n",
        "        train_loss /= n_batches\n",
        "        corr_valence /= n_batches\n",
        "        corr_arousal /= n_batches\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        val_loss = 0.0\n",
        "        n_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X = ((X - mean) / std).to(device)\n",
        "                y = y.to(device)\n",
        "                y_hat = model(X)\n",
        "                val_loss += criterion(y_hat, y).item()\n",
        "                n_batches += 1\n",
        "\n",
        "        val_loss /= n_batches\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch+1:02d} | \"\n",
        "            f\"Train MSE: {train_loss:.4f} | \"\n",
        "            f\"Val MSE: {val_loss:.4f} | \"\n",
        "            f\"Var Pearson: {corr_valence:.4f} | \"\n",
        "            f\"Aro Pearson: {corr_arousal:.4f} \"\n",
        "        )\n",
        "\n",
        "        best_val_loss = min(best_val_loss, val_loss)\n",
        "        best_val_pear = min(best_val_pear, corr_valence)\n",
        "        best_aro_pear = min(best_aro_pear, corr_aro)\n",
        "\n",
        "    fold_results.append(best_val_loss)\n",
        "    val_pear_results.append(best_val_pear)\n",
        "    aro_pear_results.append(best_aro_pear)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7866c86f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7866c86f",
        "outputId": "635e07e8-c5cb-42ab-d75d-f02cfc5c8a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-Fold CV Results:\n",
            "Mean MSE: 0.0411\n",
            "Std  MSE: 0.0031\n"
          ]
        }
      ],
      "source": [
        "fold_results = np.array(fold_results)\n",
        "val_pear_results = np.array(val_pear_results)\n",
        "aro_pear_results = np.array(aro_pear_results)\n",
        "\n",
        "print(\"10-Fold CV Results:\")\n",
        "print(f\"Mean MSE: {fold_results.mean():.4f}\")\n",
        "print(f\"Std  MSE: {fold_results.std():.4f}\")\n",
        "\n",
        "print(f\"Mean Val PCC: {val_pear_results.mean():.4f}\")\n",
        "print(f\"Std  Val PCC: {val_pear_results.std():.4f}\")\n",
        "\n",
        "print(f\"Mean Aro PCC: {aro_pear_results.mean():.4f}\")\n",
        "print(f\"Std  Aro PCC: {aro_pear_results.std():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
